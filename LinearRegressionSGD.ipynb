{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа по освоению линейной регрессии. Для этого считаю необходимым написать базовый алгоритм самостоятельно, а потом сравнить его с готовым из пакета sklearn. Для работы буду использовать модель линейной регрессии с L2 регуляризацией и стохастическим градиентным спуском в виде оптимизатора. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт необходимых пакетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация констант и генерация данных для анализа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "RANDOM_SEED = 43\n",
    "n_objects = 300\n",
    "eps = 1e-3\n",
    "\n",
    "w_true = np.random.normal(size=(n_features, ))\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "X[:, -1] = X[:, -2] + np.random.uniform(-eps, eps, X[:, -2].shape)\n",
    "Y = X.dot(w_true) + np.random.normal(0, 1, (n_objects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разделим данные на тестовые и тренировочные выборки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,test_X,train_Y,test_Y = train_test_split(X,\n",
    "                                                 Y,\n",
    "                                                 test_size = 0.33,\n",
    "                                                 random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Вывод формул"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как я и говорил выше, буду использовать L2 регуляризацию.  \n",
    "Выпишем изначальную формулу функции потерь:  \n",
    "$$\n",
    "    Q(X) = ||Y-Xw||_2^2 + \\lambda^2||w||_2^2\n",
    "$$\n",
    "Преобразуем данную формулу:  \n",
    "$$\n",
    "    Q(X) = (Y - Xw)^T(Y-Xw) + \\lambda^2(w)^T(w)  \\rightarrow min  \n",
    "$$\n",
    "\n",
    "$$\n",
    "    Q(X) = Y^TY - (Xw)^TY - Y^T(Xw) + (Xw)^T(Xw) + \\lambda^2(w)^T(w) \\rightarrow min  \n",
    "$$\n",
    "Найдем производную:\n",
    "$$\n",
    "    \\nabla_w Q(X) = 0 - XY^T - XY^T + 2XX^Tw^T + 2\\lambda^2w^T\n",
    "$$\n",
    "Прировняем к нулю и выразим вектор весов:  \n",
    "\n",
    "$$  \n",
    "    - 2XY^T  + 2XX^Tw^T + 2\\lambda^2w^T = 0  \n",
    "$$  \n",
    "Упростив уравнение продолжим выражать вектор: \n",
    "\n",
    "$$  \n",
    "    -XY^T  + XX^Tw^T + \\lambda^2w^T = 0  \n",
    "$$  \n",
    "  \n",
    "$$  \n",
    "     XX^Tw^T + \\lambda^2w^T = XY^T  \n",
    "$$  \n",
    "  \n",
    "$$  \n",
    "     (XX^T + \\lambda^2)w^T = XY^T  \n",
    "$$  \n",
    "  \n",
    "$$  \n",
    "     w^T = (XX^T + \\lambda^2)^{-1}XY^T  \n",
    "$$  \n",
    "  \n",
    "$$  \n",
    "     w = (X^TX + \\lambda^2)^{-1}X^TY  \n",
    "$$  \n",
    "\n",
    "Теперь выпишу формулу градиентного спуска:  \n",
    "$$  \n",
    "    w^{(t+1)} = w^{(t)} - \\eta  \\nabla Q(w^{(t)})  \n",
    "$$  \n",
    "Где Q(w): \n",
    "$$  \n",
    "    Q(w) =  2((X^TX + \\lambda^2)w- X^TY)  \n",
    "$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionSGD(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"LinearRegression with L2 regularization and SGD optimizer\n",
    "    \"\"\"\n",
    "    def __init__(self,alpha: float= 0.0001,\n",
    "                 max_steps = 1000,\n",
    "                 lr = 1e-2, \n",
    "                 batch_size = 50,\n",
    "                 random_seed = 43,\n",
    "                ):\n",
    "        self.alpha = alpha\n",
    "        self.max_steps = max_steps\n",
    "        self.lr =  lr\n",
    "        self.batch_size = batch_size\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "    def score(self,design_matrix, target):\n",
    "        return (linalg.norm(target-design_matrix.dot(self.w), ord = 2))**2\n",
    "        \n",
    "    def fit(self,design_matrix, target):\n",
    "        w = np.random.randn(design_matrix.shape[1])\n",
    "        step = 0\n",
    "        while (self.max_steps >= step):\n",
    "            slice_num = np.random.randint(design_matrix.shape[0],size = self.batch_size)\n",
    "            w -= 2 * self.lr * (np.dot(np.dot(design_matrix[slice_num].T,design_matrix[slice_num])+(self.alpha**2)*np.eye(design_matrix[slice_num].shape[1]),w) - np.dot(design_matrix[slice_num].T,target[slice_num])) / target[slice_num].shape[0]\n",
    "            step += 1\n",
    "        self.w = w\n",
    "        return self\n",
    "    \n",
    "    def predict(self,design_matrix):\n",
    "        return design_matrix@self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(),LinearRegressionSGD(max_steps = 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('linearregressionsgd',\n",
       "                 LinearRegressionSGD(alpha=0.0001, batch_size=50, lr=0.01,\n",
       "                                     max_steps=1000, random_seed=43))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = pipe.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_handmade = sum((test_Y - predict)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализуем тот же алгорим, но только при помощи пакетных методов sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_pipe = make_pipeline(StandardScaler(),SGDRegressor(max_iter = 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('sgdregressor',\n",
       "                 SGDRegressor(alpha=0.0001, average=False, early_stopping=False,\n",
       "                              epsilon=0.1, eta0=0.01, fit_intercept=True,\n",
       "                              l1_ratio=0.15, learning_rate='invscaling',\n",
       "                              loss='squared_loss', max_iter=1000,\n",
       "                              n_iter_no_change=5, penalty='l2', power_t=0.25,\n",
       "                              random_state=None, shuffle=True, tol=0.001,\n",
       "                              validation_fraction=0.1, verbose=0,\n",
       "                              warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package_pipe.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_package = package_pipe.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_package = sum((test_Y - predict_package)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим разницу между предсказаниями сделанными самописным алгритмом и классом SGDRegressor из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка полученная на предсказаниях проведенныйх ручным алгоритмом 87.693 и алгоритмом sklearn 86.339\n",
      "Разница между результатами составила: 1.35\n"
     ]
    }
   ],
   "source": [
    "print('Ошибка полученная на предсказаниях проведенныйх ручным алгоритмом {:.3f} и алгоритмом sklearn {:.3f}'.format(loss_handmade, loss_package))\n",
    "print(\"Разница между результатами составила: {:.3}\".format(abs(loss_handmade - loss_package)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
